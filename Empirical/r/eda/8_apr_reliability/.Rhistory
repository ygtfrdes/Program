## Define constants.
T = 5000
sigma = 0.5
T50 = 20000
## Find PDF values.
PDF = dlnorm(T,sdlog=sigma, meanlog=log(T50))
PDF
## Find CDF values.
CDF = plnorm(T,sdlog=sigma, meanlog=log(T50))
CDF
## Find failure rate.
HAZ = dlnorm(T, sdlog=sigma,
meanlog=log(T50))/(1-plnorm(T,sdlog=sigma, meanlog=log(T50)))
HAZ
## Generate 100 lognormal random numbers for probability plot.
sample=rlnorm(100, meanlog=log(20000), sdlog=0.5)
## Generate lognormal probability plot.
require(lattice)
qqmath(sample, distribution=function(p) qlnorm(p,sdlog=0.5),
ylab="TIME", xlab="EXPECTED (NORMALIZED) VALUES" ,type="l")
source('~/S11/code/R/src/8_apr_reliability/apr165.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/apr161.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/1_9_Modeling reliability growth/apr191.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/apr121.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/apr161_Exponential distribution.r', echo=TRUE)
## Evaluate the PDF at 100 hours for an exponential with lambda = 0.01.
dexp(100,0.01)
## Evaluate the CDF at 100 hours for an exponential with lambda = 0.01.
pexp(100,0.01)
## Generate 100 random exponential values using lambda = 0.01.
Y = rexp(100,0.01)
## Generate theoretical quantiles of the exponential distribution.
library(MASS)
p = fitdistr(Y,"exponential")
simdata <- qexp(ppoints(length(Y)), rate=p$estimate)
## Generate probability plot.
qqplot(simdata, Y, xlab="Theoretical Quantiles")
abline(a=0, b=1, lty=3)
## Evaluate the PDF at 100 hours for an exponential with lambda = 0.01.
dexp(100,0.01)
## Evaluate the CDF at 100 hours for an exponential with lambda = 0.01.
pexp(100,0.01)
## Generate 100 random exponential values using lambda = 0.01.
Y = rexp(100,0.01)
Y
source('~/S11/code/R/src/8_apr_reliability/apr121.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/apr121.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/apr250.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/apr413.r', echo=TRUE)
## Example posterior gamma inverse CDF probabilities.
p09 = 1/qgamma(0.9, shape=4, scale=1/3309)
p09
p08 = 1/qgamma(0.8, shape=4, scale=1/3309)
p08
p05 = 1/qgamma(0.5, shape=4, scale=1/3309)
p05
p01 = 1/qgamma(0.1, shape=4, scale=1/3309)
p01
## Generate data for plotting.
prob = seq(0.1,1,0.001)
obj = 1/qgamma(prob, shape=4, scale=1/3309)
## Plot posterior probabilities.
plot(obj,prob, type="l", xlab="MTBF Objective",
ylab="Probability of Exceeding Objective",
main="Posterior (after test) Gamma Distribution Plot")
abline(v=p09, col="blue")
abline(v=p08, col="blue")
abline(v=p05, col="blue")
abline(v=p01, col="blue")
## Compute the percentile for the reciprocal mean.
pgamma(4/3309, shape=4, scale=1/3309)
###> [1] 0.5665299
T = 100000
T50 = 507383
sigma = 0.74
y = plnorm(T/T50, sdlog=sigma)
y
T50STRESS = T*qlnorm(p, sdlog=sigma)
CDF = plnorm((100000/(A*T50STRESS)), sdlog=sigma)
ASTRESS = T*qweibull(p, shape=gamma)
T = 100000
T50 = 507383
sigma = 0.74
y = plnorm(T/T50, sdlog=sigma)
y
T50STRESS = T*qlnorm(p, sdlog=sigma)
CDF = plnorm((100000/(A*T50STRESS)), sdlog=sigma)
source('~/S11/code/R/src/8_apr_reliability/apr430.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/apr413.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/apr166_Birnbaum-Saunders distribution.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/apr166_Birnbaum-Saunders distribution.r', echo=TRUE)
source('~/S11/code/R/src/8_apr_reliability/apr166_Birnbaum-Saunders distribution.r', echo=TRUE)
## Generate probability plot.
qqmath(data.bs, distribution=function(p) qbisa(p, shape=2),
ylab="Time", xlab="Expected Value")
## Generate 100 random numbers from the Birnbaum-Saunders
## distribution.
data.bs= rbisa (100, shape=2, scale=5000)
## Load lattice package for probability plot.
require(lattice)
## Generate probability plot.
qqmath(data.bs, distribution=function(p) qbisa(p, shape=2),
ylab="Time", xlab="Expected Value")
## Functions to estimate scale and shape parameters from data.
harmon=function(x) {1/(mean(1/x))}
scale.mm = function(y) {sqrt(mean(y)*harmon(y))}
shape.mm= function(y) {sqrt(2*sqrt(mean(y)/harmon(y))-1)}
## Calculate shape parameter.
shape.est=shape.mm(data.bs)
shape.est
## Calculate scale parameter.
scale.mm(data.bs)
## Generate probability plot.
qqmath(data.bs, distribution=function(p) qbisa(p, shape=shape.est))
source('~/S11/code/R/src/8_apr_reliability/apr166_Birnbaum-Saunders distribution.r', echo=TRUE)
source('~/S11/code/r/src/8_apr_reliability/apr162_Weibull distribution.r', echo=TRUE)
source('~/S11/code/r/src/8_apr_reliability/apr161_Exponential distribution.r', echo=TRUE)
file.edit(".Rprofile")
file.edit(file.path("~", ".Rprofile"))
help("Rprofile")
.libPaths()
## Evaluate the PDF at 100 hours for an exponential with lambda = 0.01.
dexp(100,0.01)
## Evaluate the CDF at 100 hours for an exponential with lambda = 0.01.
pexp(100,0.01)
## Generate 100 random exponential values using lambda = 0.01.
Y = rexp(100,0.01)
Y
## Generate theoretical quantiles of the exponential distribution.
library(MASS)
p = fitdistr(Y,"exponential")
simdata <- qexp(ppoints(length(Y)), rate=p$estimate)
## Generate probability plot.
qqplot(simdata, Y, xlab="Theoretical Quantiles")
abline(a=0, b=1, lty=3)
##  Load VGAM package.
require(VGAM)
## Define constants gamma and mu.
mu = 5000
gamma = 2
t = 4000
## Compute the PDF at t=4000.
PDF = dbisa(t, shape=gamma, scale=mu)
PDF
## Compute the CDF.
CDF = pbisa(t, shape=gamma, scale=mu)
CDF
## Generate 100 random numbers from the Birnbaum-Saunders
## distribution.
data.bs= rbisa (100, shape=2, scale=5000)
## Load lattice package for probability plot.
require(lattice)
## Generate probability plot.
qqmath(data.bs, distribution=function(p) qbisa(p, shape=2),
ylab="Time", xlab="Expected Value")
## Functions to estimate scale and shape parameters from data.
harmon=function(x) {1/(mean(1/x))}
scale.mm = function(y) {sqrt(mean(y)*harmon(y))}
shape.mm= function(y) {sqrt(2*sqrt(mean(y)/harmon(y))-1)}
## Calculate shape parameter.
shape.est=shape.mm(data.bs)
shape.est
## Calculate scale parameter.
scale.mm(data.bs)
## Generate probability plot.
qqmath(data.bs, distribution=function(p) qbisa(p, shape=shape.est))
## Input temperature data, number of units, and cell data.
temp = c(85, 105, 125)
nu = c(100, 50, 25)
cell1 = c(401, 428, 695, 725, 738)
cell2 = c(171, 187, 189, 266, 275, 285, 301, 302, 305, 316, 317,
324, 349, 350, 386, 405, 480, 493, 530, 534, 536, 567,
589, 598, 599, 614, 620, 650, 668, 685, 718, 795, 854,
917, 926)
cell3 = c(24, 42, 92, 93, 141, 142, 143, 159, 181, 188, 194, 199,
207, 213, 243, 256, 259, 290, 294, 305, 392, 454, 502, 696)
## Apply ln function to cell data.
y1 = log(cell1)
y2 = log(cell2)
y3 = log(cell3)
## Generate lognormal probability plot using procedure from 8.2.2.1.
pos1 = 1:length(cell1)
pos2 = 1:length(cell2)
pos3 = 1:length(cell3)
pos1 = (pos1-0.3)/(nu[1]+0.4)
pos2 = (pos2-0.3)/(nu[2]+0.4)
pos3 = (pos3-0.3)/(nu[3]+0.4)
x1 = qnorm(pos1)
x2 = qnorm(pos2)
x3 = qnorm(pos3)
## Generate lognormal probability plot for each cell
## and plot the curves on the same plot.
plot(c(x1,x2,x3), c(y1,y2,y3), type="n",
xlab="Theoretical Quantiles", ylab="ln Time",
main="PROBABILITY PLOT OF TEMPERATURE CELLS")
lines(x1,y1, col="blue")
lines(x2,y2, col="blue")
lines(x3,y3, col="blue")
## Compute Ao, the ln T50 estimate, and A1, the cell sigma estimate.
z1 = lsfit(x1,y1)
z2 = lsfit(x2,y2)
z3 = lsfit(x3,y3)
## Save intercepts from the three fits.
YARRH = c(z1$coef[1], z2$coef[1], z3$coef[1])
YARRH
## Compute 11605/(temp+273.16) for three cell temperatures.
XARRH = 11605/(temp + 273.15)
XARRH
## Plot Arrhenius cell T50's.
plot(XARRH, YARRH, type="o", ylab="ln T50", xlab="11605/(t+273.16)",
main="ARRHENIUS PLOT", pch=19, col="red")
## Fit linear model.
z = lm( YARRH~XARRH,
weights=c(length(cell1), length(cell2), length(cell3)))
coef(z)
## Estimate A.
A = exp(z$coef[1])
names(A) <- NULL
A
## Estimate delta H.
dH = z$coef[2]
names(dH) <- NULL
dH
## Compute acceleration between 85 C and 125 C.
exp(dH*11605*(1/(temp[1]+273.16) - 1/(temp[3]+273.16)))
## Example of fitting a model with two stresses,
## Read data.
fname <- "../../res/jahanmi2.dat"
m <- matrix(scan(fname,skip=50),ncol=16,byrow=T)
y = m[,5]
x1 = m[,6]
x2 = m[,7]
x3 = m[,8]
x4 = m[,9]
lab = m[,2]
batch = m[,14]
## Compute summary statistics.
ybar = round(mean(y),5)
std = round(sd(y),5)
n = round(length(y),0)
stderr = round(std/sqrt(n),5)
v = round(var(y),5)
## Compute the five number summary.
## min, lower hinge, Median, upper hinge, max
z = fivenum(y)
lhinge = round(z[2],5)
uhinge = round(z[4],5)
rany = round((max(y)-min(y)),5)
## Compute the inter-quartile range.
iqry = round(IQR(y),5)
## Compute the lag 1 autocorrelation.
z = acf(y)
ac = round(z$acf[2],5)
## Format and print results.
Statistics = c(n,ybar,std,stderr,v,rany,lhinge,uhinge,iqry,ac)
names(Statistics)= c("Number of Observations ", "Mean", "Std. Dev.",
"Std. Dev. of Mean", "Variance", "Range",
"Lower Hinge", "Upper Hinge", "Inter-Quartile Range",
"Autocorrelation")
data.frame(Statistics)
summary(y)
## Generate a 4-plot of the data.
library(Hmisc)
par(mfrow = c(2, 2),
oma = c(0, 0, 2, 0),
mar = c(5.1, 4.1, 2.1, 2.1))
plot(y,ylab="Y",xlab="Run Sequence")
plot(y,Lag(y),xlab="Y[i-1]",ylab="Y[i]")
hist(y,main="",xlab="Y")
qqnorm(y,main="")
mtext("Strength of Ceramic Material: 4-Plot", line = 0.5, outer = TRUE)
par(mfrow=c(1,1))
## Generate bihistogram.
library(Hmisc)
histbackback(split(y,batch),ylab="Strength of Ceramic",
brks=seq(300,900,by=25))
## Generate a quantile-quantile plot for the two batches.
df = data.frame(y,batch)
y1 = df[batch==1,1]
y2 = df[batch==2,1]
qqplot(y2,y1)
abline(0,1)
title(sub="Quantile-Quantile Plot Y1 Y2")
## Generate a box plot for each batch.
boxplot(y~batch,ylab="Ceramic Strength",xlab="Batch")
title("Box Plot by Batch")
## Save variables as factors.
fx1 = factor(x1)
fx2 = factor(x2)
fx3 = factor(x3)
flab = factor(lab)
fbatch = factor(batch)
df = data.frame(y,fbatch,flab,fx1,fx2,fx3)
## Generate four plots on one page.
par(mfrow=c(2,2))
## Plot #1
## Compute the batch means for each laboratory.
avg = aggregate(x=df$y, by=list(df$flab,df$fbatch), FUN="mean")
## Generate the block plot.
boxplot(avg$x ~ avg$Group.1, medlty="blank",
ylab="Ceramic Strength",xlab="Laboratory",
main="Batch Means for Each Laboratory")
## Add labels for the batch means.
text(avg$Group.1[avg$Group.2==1], avg$x[avg$Group.2==1],
labels=avg$Group.2[avg$Group.2==1], pos=1)
text(avg$Group.1[avg$Group.2==2], avg$x[avg$Group.2==2],
labels=avg$Group.2[avg$Group.2==2], pos=3)
## Plot #2
## Generate the block plot for the first factor.
## Create new variable that indicates batch*x1.
## The new variable is necessary to preserve the order
## of the blocks within laboratory.
newx1 = x1/5
lx1 = factor(lab + newx1)
df = data.frame(y,fbatch,lx1)
## Compute the batch means for each laboratory and level of x1.
avg = aggregate(x=df$y, by=list(df$fbatch,df$lx1), FUN="mean")
## Read data.
m <- matrix(scan("/home/aghiles/aghiles/code/R/Handbook/res/jahanmi2.dat",skip=50),ncol=16,byrow=T)
y = m[,5]
x1 = m[,6]
x2 = m[,7]
x3 = m[,8]
x4 = m[,9]
lab = m[,2]
batch = m[,14]
## Compute summary statistics.
ybar = round(mean(y),5)
std = round(sd(y),5)
n = round(length(y),0)
stderr = round(std/sqrt(n),5)
v = round(var(y),5)
# Compute the five number summary.
# min, lower hinge, Median, upper hinge, max
z = fivenum(y)
lhinge = round(z[2],5)
uhinge = round(z[4],5)
rany = round((max(y)-min(y)),5)
## Compute the inter-quartile range.
iqry = round(IQR(y),5)
## Compute the lag 1 autocorrelation.
z = acf(y)
ac = round(z$acf[2],5)
## Format results for printing.
Statistics = c(n,ybar,std,stderr,v,rany,lhinge,uhinge,iqry,ac)
names(Statistics)= c("Number of Observations ", "Mean", "Std. Dev.",
"Std. Dev. of Mean", "Variance", "Range",
"Lower Hinge", "Upper Hinge", "Inter-Quartile Range",
"Autocorrelation")
data.frame(Statistics)
summary(y)
mails <- read.csv("/home/aghiles/aghiles/code/R/enron/dataset", sep = ' ')
library(igraph)
mails
g <- graph.data.frame(friend.t123,directed=TRUE)
library(RSiena)
library(igraph)
friend.t1 <- as.matrix(read.table("s50-network1.dat"))
friend.t2 <- as.matrix(read.table("s50-network2.dat"))
friend.t3 <- as.matrix(read.table("s50-network3.dat"))
friend.t123 <- array(c(friend.t1, friend.t2, friend.t3), dim=c(99, 99, 3))
## Evaluate the PDF at 100 hours for an exponential with lambda = 0.01.
dexp(100,0.01)
## Evaluate the CDF at 100 hours for an exponential with lambda = 0.01.
pexp(100,0.01)
## Generate 100 random exponential values using lambda = 0.01.
Y = rexp(100,0.01)
Y
## Generate theoretical quantiles of the exponential distribution.
library(MASS)
p = fitdistr(Y,"exponential")
simdata <- qexp(ppoints(length(Y)), rate=p$estimate)
## Generate probability plot.
qqplot(simdata, Y, xlab="Theoretical Quantiles")
abline(a=0, b=1, lty=3)
plot (Y)
## Generate theoretical quantiles of the exponential distribution.
library(MASS)
p = fitdistr(Y,"exponential")
simdata <- qexp(ppoints(length(Y)), rate=p$estimate)
## Generate probability plot.
qqplot(simdata, Y, xlab="Theoretical Quantiles")
abline(a=0, b=1, lty=3)
## Define constants.
## Shape = alpha = a.
## Scale = beta (b = 1/beta).
t = 24
a = 2
beta = 30
## Calculate PDF value.
pdf1 = dgamma(t, shape=a, scale=beta)
pdf1
## Calculate CDF value.
cdf1 = pgamma(t, shape=a, scale=beta)
cdf1
## Calculate reliability.
REL = 1-cdf1
REL
## Calculate failure rate.
FR = pdf1/REL
FR
## Generate 100 Gamma random numbers.
data1 = rgamma(100, shape=a, scale=beta)
## Load lattice library for plotting.
require(lattice)
## Generate probability plot.
qqmath(data1,distribution=function(p) qgamma(p, shape=2),
ylab="TIME" ,xlab="EXPECTED (NORMALIZED) VALUES")
## The value of the shape parameter gamma can be
## estimated with a method of moments estimator.
shape.est = (mean(data1)/sd(data1))^2
qqmath(data1, distribution=function(p) qgamma(p, shape=shape.est),
ylab="TIME" ,xlab="EXPECTED (NORMALIZED) VALUES")
## Define constants.
T = 5000
sigma = 0.5
T50 = 20000
## Find PDF values.
PDF = dlnorm(T,sdlog=sigma, meanlog=log(T50))
PDF
## Find CDF values.
CDF = plnorm(T,sdlog=sigma, meanlog=log(T50))
CDF
## Find failure rate.
HAZ = dlnorm(T, sdlog=sigma,
meanlog=log(T50))/(1-plnorm(T,sdlog=sigma, meanlog=log(T50)))
HAZ
## Generate 100 lognormal random numbers for probability plot.
sample=rlnorm(100, meanlog=log(20000), sdlog=0.5)
## Generate lognormal probability plot.
require(lattice)
qqmath(sample, distribution=function(p) qlnorm(p,sdlog=0.5),
ylab="TIME", xlab="EXPECTED (NORMALIZED) VALUES" ,type="l")
## Generate lognormal probability plot when sigma is estimated from data.
logsamp = log(sample)
SD.est = sd(logsamp)
qqmath(sample, distribution=function(p) qlnorm(p,sdlog=SD.est),
ylab="TIME", xlab="EXPECTED (NORMALIZED) VALUES", type="l")
## Load the package containing special survival analysis functions.
require(survival)
## Create survival object.
failures = c(55, 187, 216, 240, 244, 335, 361, 373, 375, 386)
y = Surv(c(failures, rep(500, 10)), c(rep(1, length(failures)), rep(0, 10)))
## Fit survival data.
ys = survfit(y ~ 1, type="kaplan-meier")
summary(ys)
## Generate Kaplan-Meier survival curve.
plot(ys, xlab="Hours", ylab="Survival Probability")
## Generate a Weibull probability plot.
p = ppoints(failures, a=0.3)
plot(failures, -log(1-p), log="xy", pch=19, col="red",
xlab="Hours", ylab="Cumulative Hazard")
## Estimate parameters for Weibull distribution.
yw = survreg(y ~ 1, dist="weibull")
summary(yw)
## Log-likelihood and Akaike's Information Criterion
signif(summary(yw)$loglik, 5)
signif(extractAIC(yw), 5)
etaHAT <- exp(coefficients(yw)[1])
betaHAT <- 1/yw$scale
signif(c(eta=etaHAT, beta=betaHAT), 6)
## Lifetime: expected value and standard deviation.
muHAT = etaHAT * gamma(1 + 1/betaHAT)
sigmaHAT = etaHAT * sqrt(gamma(1+2/betaHAT) - (gamma(1+1/betaHAT))^2)
names(muHAT) = names(sigmaHAT) = names(betaHAT) = names(etaHAT) = NULL
signif(c(mu=muHAT, sigma=sigmaHAT), 6)
## Probability density of fitted model.
curve(dweibull(x, shape=betaHAT, scale=etaHAT),
from=0, to=muHAT+6*sigmaHAT, col="blue",
xlab="Hours", ylab="Probability Density")
## Weibull versus lognormal models.
yl = survreg(y ~ 1, dist="lognormal")
signif(c(lognormalAIC=extractAIC(yl)[2], weibullAIC=extractAIC(yw)[2]), 5)
#> lognormalAIC   weibullAIC
#> lognormalAIC   weibullAIC
#>       154.33       154.24
#> lognormalAIC   weibullAIC
#>       154.33       154.24
## Input data and generate plotting variables.
## x = time of failure
## y = ln(1/(1-F(t))
x = c(54, 187, 216, 240, 244, 335, 361, 373, 375, 386)
n = 20
fnum = c(1:length(x))
Ft = (fnum-0.3)/(n+0.4)
y = log(1/(1-Ft))
## Print y values.
y
## Generate probability plot.
plot(x,y, xlab="Time", ylab="ln(1/(1-F(t)))", log="xy",
type="o", pch=19, col="blue")
## Compute slope of line fit to plotted data.
xx=log10(x)
yy=log10(y)
z = lm(yy ~ xx)
coef(z)
## Another way to generate the Weibull Probability Plot using
## functions already available in R.
p = ppoints(x, a=0.3)
plot(x, -log(1-p), log="xy", type="o", col="blue",
xlab="Time", ylab="ln(1/(1-F(t)))")
## Read data and save relevant variables.
m = matrix(scan("../../res/machine.dat",skip=25),ncol=5,byrow=T)
machine = m[,1]
day = m[,2]
time = m[,3]
sample = m[,4]
diameter = m[,5]
## Generate box plot for each machine.
df = data.frame(diameter,machine,day,time,sample)
boxplot(diameter~machine, data=df, ylab="Diameter (inches)", xlab="Machine",
main="Box Plot by Machine")
